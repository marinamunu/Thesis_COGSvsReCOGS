{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e065c2-6af6-4ea6-bcb2-b48e1d98f931",
   "metadata": {},
   "source": [
    "# TARGETED ERROR ANALYSIS\n",
    "\n",
    "This notebook contains the code used to perform a targeted error analysis on the model output. It uses the get_groupins() function as well as parts of the get_results() function code. The notebook is organised by generalisation case. For each case, the first cell defines the path to the model output file being analysed. The next cell runs a general evaluation of the results which calculates the number of cases which include index errors, missing elements, and inserted elements in the predicted LFs, as well as the overall number of incorrect predictions. This code is the same for all generalisation cases. The last cell of code is specific to the targeted focus of each gen case, checking for different specific errors depending on the requirements of the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49840c-c89a-476b-a9c7-8b77c9f60831",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5376fce-3b6d-4464-981e-5d532a1cb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groupings(lf):\n",
    "    \"\"\"\n",
    "    extract the relational groupings from an LF structure \n",
    "    - the reasoning behind this is to check indexing, which draws out these relational groupings\n",
    "\n",
    "    lf : can be either predicted or target LF string\n",
    "    \"\"\"\n",
    "    \n",
    "    import re\n",
    "    #lf = re.split(';|AND', lf_string)\n",
    "\n",
    "    rel_list = []\n",
    "    for item in lf:\n",
    "        if \",\" in item:\n",
    "            rel_list.append(item)    \n",
    "\n",
    "    for item in rel_list:\n",
    "        if item in lf:\n",
    "            lf.remove(item)\n",
    "\n",
    "    rel_groups = []\n",
    "    #print(lf)\n",
    "    \n",
    "    for relation in rel_list:\n",
    "        group = []\n",
    "        role = relation.split(\" (\")[0]\n",
    "        if '.' in role:\n",
    "            role = role.split('.')\n",
    "            group.append(role[0])\n",
    "            group.append(role[1])\n",
    "        else:\n",
    "            group.append(role)\n",
    "\n",
    "        indxs = re.findall(r'\\d+', relation)\n",
    "\n",
    "        if \" ( ( \" in relation:\n",
    "            #print(relation)\n",
    "            continue \n",
    "        if \" (\" not in relation:\n",
    "            continue \n",
    "        if len(indxs) < 1:\n",
    "            continue \n",
    "        if len(indxs) == 1:\n",
    "            ind1 = indxs[0]\n",
    "            #print(relation)\n",
    "            name = relation.split(\" (\")[1].split(',')[1]\n",
    "            name = re.findall(\"[a-zA-Z]+\", name)\n",
    "            if len(name) < 1:\n",
    "                name = relation.split(\" (\")[1].split(',')[0]\n",
    "                group.append(name)\n",
    "            else:\n",
    "                group.append(name[0])\n",
    "            for item in pred:\n",
    "                if f' {ind1} ' in item:\n",
    "                    group.append(item.split(\" (\")[0])\n",
    "        else:\n",
    "            ind1 = indxs[0]\n",
    "            ind2 = indxs[1]\n",
    "            for item in lf:\n",
    "                if f' {ind1} ' in item:\n",
    "                    group.append(item.split(\" (\")[0])\n",
    "                elif f' {ind2} ' in item:\n",
    "                    group.append(item.split(\" (\")[0])\n",
    "        #print(group)\n",
    "                \n",
    "        rel_groups.append(group)\n",
    "            \n",
    "    return rel_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3868d0c5-ac15-4adb-b822-46b31630259e",
   "metadata": {},
   "source": [
    "# Object Modification to Subject Modification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c75447e6-6a79-4b3e-9e50-c3a8fbba2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_path = \"/Users/marina/Desktop/RESULTS/Rpos_Transformr/SEED66/PRED_66_ende_transformer_recogspos_v1_cogs.tsv\"\n",
    "#gen_path = \"/Users/marina/Desktop/RESULTS/C_Transformer/SEED88/PRED_88_ende_transformer_cogs_v1_cogs.tsv\"\n",
    "gen_path = \"/Users/marina/Desktop/RESULTS/R_LSTM/SEED42/PRED_42_ende_lstm_recogs_v1_cogs.tsv\"\n",
    "#gen_path = \"/Users/marina/Desktop/RESULTS/R_Transformer/SEED66/PRED_66_ende_transformer_recogs_v1_cogs.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "987113d5-2bf4-4e77-bed6-77d2189e9a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851\n",
      "{'total': 1000, 'incorrect': 996, 'missing': 973, 'inserted': 851, 'index error': 994}\n",
      "604\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "#general checks : only predict lexical items present in input, keep track of variable naming (indexing)\n",
    "inserted = 0\n",
    "missing = 0\n",
    "mis_index = 0\n",
    "total = 0\n",
    "incorrect = 0\n",
    "inc_cases = []\n",
    "countmult=0\n",
    "nmodmis_count = 0\n",
    "for line in content_t[1:-1]:\n",
    "    #total+=1\n",
    "\n",
    "    sent = line[0]\n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "\n",
    "    if cat == ' obj_pp_to_subj_pp':\n",
    "        total+=1\n",
    "    \n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "        \n",
    "        p_elements = []\n",
    "        t_elements = []\n",
    "        \n",
    "        for item in pred:\n",
    "            item = item.split('(')\n",
    "            p_elements.append(item[0])\n",
    "        for item in target:\n",
    "            item = item.split('(')\n",
    "            t_elements.append(item[0])\n",
    "        \n",
    "        case_missing=0\n",
    "        for el in t_elements:\n",
    "            if el not in p_elements:\n",
    "                case_missing+=1\n",
    "                if 'nmod' in el:\n",
    "                    nmodmis_count +=1\n",
    "            \n",
    "        if case_missing > 0:\n",
    "            missing+=1\n",
    "            \n",
    "\n",
    "        ins_els = []\n",
    "        case_inserted=0\n",
    "        for el in p_elements:\n",
    "            if el not in t_elements:\n",
    "                case_inserted+=1\n",
    "                ins_els.append(el)\n",
    "                \n",
    "        if case_inserted > 0:\n",
    "            inserted+=1\n",
    "    \n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "        \n",
    "        case_misindex = 0\n",
    "        for group_p, group_t in zip(ind_group_pred, ind_group_target):\n",
    "            if group_p != group_t:\n",
    "                count=0\n",
    "                    \n",
    "                case_misindex +=1\n",
    "                \n",
    "        \n",
    "        if case_misindex > 0:\n",
    "            mis_index += 1\n",
    "\n",
    "        if case_misindex > 0 or case_inserted > 0 or case_missing > 0:\n",
    "            incorrect+=1\n",
    "            inc_cases.append(line)\n",
    "        \n",
    "            \n",
    "\n",
    "error_pattern = {'total': total, 'incorrect': incorrect, 'missing': missing, 'inserted': inserted, 'index error': mis_index}\n",
    "print(error_pattern)\n",
    "print(nmodmis_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b20029b-98dc-48bd-8d9e-1df9d95116a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 996\n",
      "SUBJECT -----------------------------\n",
      "SUBJ OMISSION: 0\n",
      "SUBJ DUPLICATION: 9\n",
      "RELATION -----------------------------\n",
      "RELATION ERROR: 702\n",
      "RELATION ROLE MISSING: 29\n",
      "RELATION MAIN SUBJ MISSING: 197\n",
      "RELATION VERB MISSING/MISPREDICTED: 629\n",
      "PP ELEMENT -----------------------------\n",
      "NMOD ERROR: 970\n",
      "NMOD MISSING: 524\n",
      "NMOD INSERTED: 52\n",
      "NMOD MAIN SUBJ MISSING: 969\n",
      "0\n",
      "996\n",
      "996\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "# targeted check\n",
    "missing_subj = 0  ## count number of cases where main subject is missing\n",
    "multiple_subj = 0 ## count number of cases where main subject is instantiated more than once \n",
    "inc_relation = 0  ## count number of cases where relevant relation incorrectly excludes main subject \n",
    "inc_nmod = 0   ## count number of cases where there's an nmod related mistake \n",
    "missing_nmod = 0 ## count number of cases where nmod element is missing \n",
    "missing_rel = 0 ## count number of cases where the relevant thematic relation is missing\n",
    "countsame=0\n",
    "misfirstnmod=0\n",
    "missing_verb=0\n",
    "nonmod_count = 0\n",
    "inserted_nmod = 0\n",
    "nmodmissing_subj = 0\n",
    "relmissing_subj = 0\n",
    "relmissing_verb = 0\n",
    "\n",
    "for line in inc_cases:\n",
    "    #total+=1\n",
    "    \n",
    "    sent = line[0]\n",
    "    \n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "    \n",
    "    if cat == ' obj_pp_to_subj_pp':\n",
    "\n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "\n",
    "        sent_list = sent.split(\" \")\n",
    "        main_subj = sent_list[1]\n",
    "\n",
    "        notsubj_count = 0\n",
    "        subj_count = 0\n",
    "        for item in pred:\n",
    "            if f'{main_subj} (' in item:\n",
    "                subj_count+=1\n",
    "                subj_ind = re.findall(r'\\d+', item)\n",
    "\n",
    "            else:\n",
    "                notsubj_count +=1\n",
    "                \n",
    "        if notsubj_count == len(pred):\n",
    "            missing_subj +=1 \n",
    "        if subj_count > 1:\n",
    "            multiple_subj += 1\n",
    "\n",
    "        if f' {main_subj} . nmod ' not in line[1]:\n",
    "            misfirstnmod+=1\n",
    "\n",
    "        count=0\n",
    "        for item in target:\n",
    "            if ',' in item and 'nmod' not in item:\n",
    "                count+=1\n",
    "                if count==1:\n",
    "                    verb_ind = re.findall(r'\\d+', item.split(',')[0])\n",
    "                    verb = item.split('.')[0]\n",
    "                   # for item in target: ## UNCOMMENT FOR ReCOGS\n",
    "                    #    if verb_ind[0] in item and ',' not in item:\n",
    "                     #       print(item)\n",
    "                     #       verb = item.split('(')[0]\n",
    "                #print(verb)\n",
    "\n",
    "        if verb not in line[1]:\n",
    "            missing_verb+=1\n",
    "        \n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "\n",
    "        target_nmod_count=0\n",
    "        for t_group in ind_group_target:\n",
    "            for item in t_group:\n",
    "                if main_subj in item:\n",
    "                    if ' nmod ' in t_group:\n",
    "                        nmod_group = t_group\n",
    "                    else:\n",
    "                        rel_group = t_group\n",
    "            if ' nmod ' in t_group:\n",
    "                target_nmod_count+=1\n",
    "        \n",
    "        norel_count = 0 \n",
    "        if nmod_group not in ind_group_pred:\n",
    "            inc_nmod +=1\n",
    "        if rel_group not in ind_group_pred:\n",
    "            inc_relation +=1\n",
    "\n",
    "        current_verbmiscount = relmissing_verb\n",
    "        current_relinccount = inc_relation\n",
    "        pred_nmod_count=0\n",
    "        no_subj=0\n",
    "        misverbcount = 0\n",
    "        missubjcount = 0\n",
    "        for p_group in ind_group_pred:\n",
    "            if ' nmod ' in p_group:\n",
    "                pred_nmod_count+=1\n",
    "                if f' {main_subj}' not in p_group:\n",
    "                    no_subj+=1\n",
    "            if rel_group[0] in p_group:\n",
    "                #print(ind_group_pred)\n",
    "                if f' {main_subj}' in p_group or f' * {main_subj}' in p_group :\n",
    "                   #print(rel_group)\n",
    "                   if rel_group[2] not in p_group:\n",
    "                       #relmissing_verb +=1  \n",
    "                       misverbcount +=1\n",
    "                else:\n",
    "                   #relmissing_subj+=1\n",
    "                    missubjcount+=1\n",
    "                    if rel_group[2] not in p_group:\n",
    "                       #relmissing_verb +=1\n",
    "                       misverbcount+=1\n",
    "                   \n",
    "            else:\n",
    "                norel_count+=1\n",
    "                \n",
    "\n",
    "        if misverbcount > 0:\n",
    "            relmissing_verb+=1\n",
    "        if missubjcount > 0:\n",
    "            relmissing_subj+=1\n",
    "        if no_subj == pred_nmod_count:\n",
    "            nmodmissing_subj+=1\n",
    "        if pred_nmod_count != target_nmod_count:\n",
    "            if pred_nmod_count < target_nmod_count:\n",
    "                missing_nmod+=1\n",
    "            if pred_nmod_count == 0:\n",
    "                nonmod_count+=1\n",
    "            if pred_nmod_count > target_nmod_count:\n",
    "                inserted_nmod+=1\n",
    "     \n",
    "        if norel_count == len(ind_group_pred):\n",
    "            missing_rel+=1\n",
    "        \n",
    "\n",
    "print(f'TOTAL: {len(inc_cases)}')\n",
    "print(f'SUBJECT -----------------------------')\n",
    "print(f'SUBJ OMISSION: {missing_subj}')\n",
    "print(f'SUBJ DUPLICATION: {multiple_subj}')\n",
    "print(f'RELATION -----------------------------')\n",
    "print(f'RELATION ERROR: {inc_relation}')\n",
    "print(f'RELATION ROLE MISSING: {missing_rel}')\n",
    "print(f'RELATION MAIN SUBJ MISSING: {relmissing_subj}')\n",
    "print(f'RELATION VERB MISSING/MISPREDICTED: {relmissing_verb}')\n",
    "print(f'PP ELEMENT -----------------------------')\n",
    "print(f'NMOD ERROR: {inc_nmod}')\n",
    "print(f'NMOD MISSING: {missing_nmod}')\n",
    "print(f'NMOD INSERTED: {inserted_nmod}')\n",
    "print(f'NMOD MAIN SUBJ MISSING: {nmodmissing_subj}')\n",
    "\n",
    "print(countsame)\n",
    "print(misfirstnmod)\n",
    "print(missing_verb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327e162-06e2-4def-86f5-049e6c377a3f",
   "metadata": {},
   "source": [
    "# PP Recurssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "944dcf01-bade-472c-b9e3-b9cb3d78250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = \"/Users/marina/Desktop/RESULTS/R_LSTM/SEED42/PRED_42_ende_lstm_recogs_v1_cogs.tsv\"\n",
    "#gen_path = \"/Users/marina/Desktop/RESULTS/Rpos_LSTM/SEED88/PRED_88_ende_lstm_recogspos_v1_cogs.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6da1e85d-5f30-4217-86b0-2e271c7930c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 1000, 'incorrect': 1000, 'missing': 983, 'inserted': 760, 'index error': 1000}\n",
      "2302\n",
      "856\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "#general checks : only predict lexical items present in input, keep track of variable naming (indexing)\n",
    "inserted = 0\n",
    "missing = 0\n",
    "mis_index = 0\n",
    "total = 0\n",
    "incorrect = 0\n",
    "inc_cases = []\n",
    "countmult=0\n",
    "total_countnmod=0\n",
    "countnmod=0\n",
    "count_insnmod=0\n",
    "ccount=0\n",
    "\n",
    "for line in content_t[1:-1]:\n",
    "    #total+=1\n",
    "\n",
    "    sent = line[0]\n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "\n",
    "    if cat == ' pp_recursion':\n",
    "        total+=1\n",
    "    \n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "        \n",
    "        p_elements = []\n",
    "        t_elements = []\n",
    "        \n",
    "        for item in pred:\n",
    "            item = item.split('(')\n",
    "            p_elements.append(item[0])\n",
    "        for item in target:\n",
    "            item = item.split('(')\n",
    "            t_elements.append(item[0])\n",
    "        \n",
    "        case_missing=0\n",
    "        missingnmod=0\n",
    "        for el in t_elements:\n",
    "            if el not in p_elements:\n",
    "                case_missing+=1\n",
    "                if 'nmod' in el:\n",
    "                    #if 'hippo' not in line[1]:\n",
    "                    total_countnmod+=1\n",
    "                    missingnmod+=1\n",
    "       \n",
    "        if case_missing > 0:\n",
    "            missing+=1\n",
    "        if missingnmod > 0:\n",
    "            countnmod+=1\n",
    "            \n",
    "\n",
    "        ins_els = []\n",
    "        case_inserted=0\n",
    "        ins_countnmod=0\n",
    "        for el in p_elements:\n",
    "            if el not in t_elements:\n",
    "                case_inserted+=1\n",
    "                ins_els.append(el)\n",
    "                if 'nmod' in el:\n",
    "                    ins_countnmod+=1\n",
    "                else:\n",
    "                    if '.' in el:\n",
    "                        ccount+=1\n",
    "             \n",
    "\n",
    "       # if len(p_elements) > len(t_elements):\n",
    "        #    print(p_elements)\n",
    "         #   print(t_elements)\n",
    "          #  print()\n",
    "\n",
    "        if ins_countnmod > 0:\n",
    "            count_insnmod+=1\n",
    "        if case_inserted > 0:\n",
    "            inserted+=1\n",
    "\n",
    "        #print(ins_els)  ## to check if there are more than one inserted element in prediction (i.e. more than just the subj element)\n",
    "        #if len(ins_els)>1:\n",
    "         #   print(sent)\n",
    "          #  print(pred)\n",
    "           # print(target)\n",
    "    \n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "        \n",
    "        case_misindex = 0\n",
    "        for group_p, group_t in zip(ind_group_pred, ind_group_target):\n",
    "            if group_p != group_t:\n",
    "                count=0\n",
    "         \n",
    "                case_misindex +=1\n",
    "                \n",
    "        if case_misindex > 1:\n",
    "            countmult+=1\n",
    "        if case_misindex > 0:\n",
    "            mis_index += 1\n",
    "       \n",
    "        if case_misindex > 0 or case_inserted > 0 or case_missing > 0:\n",
    "            incorrect+=1\n",
    "            inc_cases.append(line)\n",
    "       \n",
    "            \n",
    "\n",
    "error_pattern = {'total': total, 'incorrect': incorrect, 'missing': missing, 'inserted': inserted, 'index error': mis_index}\n",
    "#print(countmult)\n",
    "print(error_pattern)\n",
    "print(total_countnmod)\n",
    "print(countnmod)\n",
    "print(count_insnmod)\n",
    "print(ccount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3c05f63-fb02-476a-b3a4-6357e78d8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overview of targeted check\n",
    "\n",
    "## make a list of nouns in target LF (i.e. identify all elements in LF without ',', so only one index, and put in a list)\n",
    "#   problem with recogs LF because verbs will also appear here, can avoid by looking for agent element, verb will always be mapped\n",
    "##  to an agent element, find the agent in target LF, find the item its mapped to, remove this from the list of nouns\n",
    "\n",
    "## check that all nouns from list are present in predicted\n",
    "#   check for nmod relations by identifying all nmods (with their prepositions) in target LF, identifying the first index its mapped to\n",
    "##  checking the noun this corresponds to and then making this check in predicted LF to make sure its the same\n",
    "\n",
    "## as an extra check, check the theme element in the sentence and that its mapped to correct object\n",
    "#   again, identify theme in target, identify object its mapped to, check for this mapping in predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ffa069c-ba78-4085-8fea-26e5b9becc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 1000\n",
      "NP -----------------------------\n",
      "NOUN ERROR: 969\n",
      "NOUN MISSING: 919\n",
      "NOUN INSERTED: 0\n",
      "NMOD RELATION -----------------------------\n",
      "NMOD ERROR: 1000\n",
      "NMOD MISSING: 1000\n",
      "NMOD INSETED: 0\n",
      "NMOD MAPPING ERROR: 973\n",
      "AVERAGE CORRECT NMOD ELEMENT 2.007 AND INCORRECT 0.002\n",
      "THEME RELATION -----------------------------\n",
      "THEME ERROR: 1000\n"
     ]
    }
   ],
   "source": [
    "# import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "# targeted check\n",
    "noun_error = 0 ## count number of cases where there is an error with the predicted nouns\n",
    "missing_noun = 0 ## count nunber of cases where a noun from the different levels of PP is missing\n",
    "inserted_noun = 0 ## count number of cases where a noun not from input is inserted\n",
    "nmod_error = 0 ## count number of cases where there is an error with the predicted nmod relations\n",
    "missing_nmod = 0 ## count number of cases where an nmod relation is missing\n",
    "inc_nmodmap = 0 ## count number of cases where an nmod relation is not mapped to correct noun\n",
    "inserted_nmod = 0 ## count number of cases where an nmod relation is incorrectly inserted\n",
    "theme_error = 0 ## count number of cases where the theme element is incorrect, not mapped to correct object element\n",
    "\n",
    "correctnmod = [] ## append number of correct nmod element predictions per case\n",
    "incorrectnmod = [] ## append number of incorrect nmod element predictions per case\n",
    "\n",
    "for line in inc_cases:\n",
    "    #total+=1\n",
    "    \n",
    "    sent = line[0]\n",
    "    \n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "    \n",
    "    if cat == ' pp_recursion':\n",
    "\n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "\n",
    "        noun_list = []\n",
    "        nmod_list = []\n",
    "        for el in target:\n",
    "            if ',' not in el:\n",
    "                noun_list.append(el.split('(')[0])\n",
    "            elif 'agent' in el:\n",
    "                if '.' in el:\n",
    "                    verb = el.split('.')[0]\n",
    "                else:\n",
    "                    el = el.split(',')[0]\n",
    "                    verb_ind = re.findall(r'\\d+', el)\n",
    "                    for el in target:\n",
    "                        if ',' not in el and verb_ind[0] in el:\n",
    "                            verb = el.split('(')[0]\n",
    "            elif 'nmod' in el:\n",
    "                nmod_list.append(el.split('(')[0])\n",
    "            elif 'theme' in el:\n",
    "                el = el.split(',')[1]\n",
    "                obj_ind = re.findall(r'\\d+', el)\n",
    "                for el in target:\n",
    "                    if ',' not in el and obj_ind[0] in el:\n",
    "                        obj = el.split('(')[0]\n",
    "\n",
    "        \n",
    "        if verb[0] in noun_list:\n",
    "            noun_list.remove(verb[0])\n",
    "\n",
    "        pred_nounlist = []\n",
    "        pred_nmodlist = []\n",
    "        for el in pred:\n",
    "            if ',' not in el:\n",
    "                pred_nounlist.append(el.split('(')[0])\n",
    "            elif 'agent' in el:\n",
    "                if '.' in el:\n",
    "                    verb = el.split('.')[0]\n",
    "                else:\n",
    "                    el = el.split(',')[0]\n",
    "                    verb_ind = re.findall(r'\\d+', el)\n",
    "                    for el in pred:\n",
    "                        if ',' not in el and verb_ind[0] in el:\n",
    "                            verb = el.split('(')[0]\n",
    "            elif 'nmod' in el:\n",
    "                pred_nmodlist.append(el.split('(')[0]) \n",
    "            elif 'theme' in el:\n",
    "                el = el.split(',')[1]\n",
    "                mapped_obj = re.findall(r'\\d+', el)\n",
    "                if len(mapped_obj) > 0:\n",
    "                    for el in pred:\n",
    "                        if ',' not in el and mapped_obj[0] in el:\n",
    "                            pred_obj = el\n",
    "        if verb[0] in noun_list:\n",
    "            pred_nounlist.remove(verb[0])\n",
    "\n",
    "        pred_nounlist.sort()\n",
    "        noun_list.sort()\n",
    "        pred_nmodlist.sort()\n",
    "        nmod_list.sort()\n",
    "        \n",
    "        if pred_nounlist != noun_list:\n",
    "            noun_error+=1\n",
    "            if len(pred_nounlist) > len(noun_list):\n",
    "                inserted_noun+=1\n",
    "            elif len(pred_nounlist) < len(noun_list):\n",
    "                missing_noun+=1\n",
    "\n",
    "        if pred_nmodlist != nmod_list:\n",
    "            nmod_error+=1\n",
    "            if len(pred_nmodlist) > len(nmod_list):\n",
    "                inserted_nmod+=1\n",
    "            if len(pred_nmodlist) < len(nmod_list):\n",
    "                missing_nmod+=1\n",
    "           \n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "        \n",
    "        correct_prednmod=0\n",
    "        incorrect_prednmod=0\n",
    "        nmod_maperror=0\n",
    "        nmodstring = ''.join(nmod_list)\n",
    "        for item in pred_nmodlist:\n",
    "            if item in nmodstring:\n",
    "                correct_prednmod+=1\n",
    "                itemlist = item.split('.') ## for COGS LF structure\n",
    "                #itemlist.remove(itemlist[-1])\n",
    "                #itemstring = ''.join(itemlist)\n",
    "                itemstring = item\n",
    "                for group in ind_group_pred:\n",
    "                    groupstring = ''.join(group)\n",
    "                   # if itemstring in groupstring:      \n",
    "                    if len(itemlist) > 1:\n",
    "                        if itemlist[0] in groupstring and itemlist[1] in groupstring:\n",
    "                            p_grouping = group\n",
    "                for tgroup in ind_group_target:\n",
    "                    t_groupstring = ''.join(tgroup)\n",
    "                    if len(itemlist) > 1:\n",
    "                        if itemlist[0] in t_groupstring and itemlist[1] in t_groupstring:\n",
    "                   # if itemstring in t_groupstring:\n",
    "                            t_grouping = tgroup\n",
    "                if len(itemlist) > 1:\n",
    "                    if p_grouping != t_grouping:\n",
    "                        nmod_maperror+=1\n",
    "            else:\n",
    "                incorrect_prednmod+=1\n",
    "        if nmod_maperror > 0:\n",
    "            inc_nmodmap+=1\n",
    "        \n",
    "        correctnmod.append(correct_prednmod)\n",
    "        incorrectnmod.append(incorrect_prednmod)\n",
    "        \n",
    "        if obj != pred_obj:\n",
    "            theme_error+=1\n",
    "        if 'theme' not in line[1]:\n",
    "            theme_error+=1\n",
    "       \n",
    "    \n",
    "\n",
    "avg_correctnmod = sum(correctnmod)/len(correctnmod)\n",
    "avg_incorrectnmod = sum(incorrectnmod)/len(incorrectnmod)\n",
    "\n",
    "print(f'TOTAL: {len(inc_cases)}')\n",
    "print(f'NP -----------------------------')\n",
    "print(f'NOUN ERROR: {noun_error}')\n",
    "print(f'NOUN MISSING: {missing_noun}')\n",
    "print(f'NOUN INSERTED: {inserted_noun}')\n",
    "print(f'NMOD RELATION -----------------------------')\n",
    "print(f'NMOD ERROR: {nmod_error}')\n",
    "print(f'NMOD MISSING: {missing_nmod}')\n",
    "print(f'NMOD INSETED: {inserted_nmod}')\n",
    "print(f'NMOD MAPPING ERROR: {inc_nmodmap}')\n",
    "print(f'AVERAGE CORRECT NMOD ELEMENT {avg_correctnmod} AND INCORRECT {avg_incorrectnmod}')\n",
    "print(f'THEME RELATION -----------------------------')\n",
    "print(f'THEME ERROR: {theme_error}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145faef7-a58e-4124-af61-504fbaa92ef1",
   "metadata": {},
   "source": [
    "# Theme NP to Objected Omitted Transitive Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e63e91-7970-4c0f-8b82-5b6b3a3d7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_path = \"/Users/marina/Desktop/RESULTS/C_LSTM/SEED66/PRED_66_ende_lstm_cogs_v1_cogs.tsv\"\n",
    "gen_path = \"/Users/marina/Desktop/FullReCOGS/[100Epoch]LSTM_77/PRED_77_ende_lstm_recogs_v1_cogs.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aac3d6d-0c46-4fbb-adfd-99e3d5925843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 1000, 'incorrect': 194, 'missing': 174, 'inserted': 174, 'index error': 194}\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "#general checks : only predict lexical items present in input, keep track of variable naming (indexing)\n",
    "inserted = 0\n",
    "missing = 0\n",
    "mis_index = 0\n",
    "total = 0\n",
    "incorrect = 0\n",
    "inc_cases = []\n",
    "countmult=0\n",
    "counthip=0\n",
    "\n",
    "for line in content_t[1:-1]:\n",
    "    #total+=1\n",
    "\n",
    "    sent = line[0]\n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "\n",
    "    if cat == ' only_seen_as_unacc_subj_as_obj_omitted_transitive_subj':\n",
    "        total+=1\n",
    "    \n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "        \n",
    "        p_elements = []\n",
    "        t_elements = []\n",
    "        \n",
    "        for item in pred:\n",
    "            item = item.split('(')\n",
    "            p_elements.append(item[0])\n",
    "        for item in target:\n",
    "            item = item.split('(')\n",
    "            t_elements.append(item[0])\n",
    "        \n",
    "        case_missing=0\n",
    "        for el in t_elements:\n",
    "            if el not in p_elements:\n",
    "                case_missing+=1\n",
    "                if 'hip' in el:\n",
    "                    #if 'hippo' not in line[1]:\n",
    "                    counthip+=1\n",
    "        \n",
    "        if case_missing > 0:\n",
    "            missing+=1\n",
    "            \n",
    "\n",
    "        ins_els = []\n",
    "        case_inserted=0\n",
    "        for el in p_elements:\n",
    "            if el not in t_elements:\n",
    "                case_inserted+=1\n",
    "                ins_els.append(el)\n",
    "        \n",
    "        if case_inserted > 0:\n",
    "            inserted+=1\n",
    "\n",
    "        #print(ins_els)  ## to check if there are more than one inserted element in prediction (i.e. more than just the subj element)\n",
    "        #if len(ins_els)>1:\n",
    "         #   print(sent)\n",
    "          #  print(pred)\n",
    "           # print(target)\n",
    "    \n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "        \n",
    "        case_misindex = 0\n",
    "        for group_p, group_t in zip(ind_group_pred, ind_group_target):\n",
    "            if group_p != group_t:\n",
    "                count=0\n",
    "         \n",
    "                case_misindex +=1\n",
    "                \n",
    "        if case_misindex > 1:\n",
    "            countmult+=1\n",
    "        if case_misindex > 0:\n",
    "            mis_index += 1\n",
    "    \n",
    "       \n",
    "        if case_misindex > 0 or case_inserted > 0 or case_missing > 0:\n",
    "            incorrect+=1\n",
    "            #inc_cases.append(line)\n",
    "            #print(sent)\n",
    "            \n",
    "\n",
    "error_pattern = {'total': total, 'incorrect': incorrect, 'missing': missing, 'inserted': inserted, 'index error': mis_index}\n",
    "#print(countmult)\n",
    "print(error_pattern)\n",
    "print(counthip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d957569-3dbd-4b49-b0b2-176d1ceafd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overview of Targeted Check\n",
    "\n",
    "### targeted check the main subject element (which has only been seen as an unaccusative object) and its agent linked element\n",
    "##  can also check for any theme elements to make sure the main subject isn't being linked to those \\\n",
    "\n",
    "## main subject always 'hippo' (only seen once in training so can except misprediction) \n",
    "\n",
    "# Olivia wished that a hippo dusted .\t\n",
    "# wish . agent ( x _ 1 , Olivia ) AND wish . ccomp ( x _ 1 , x _ 5 ) AND hippo ( x _ 4 ) AND dust . agent ( x _ 5 , x _ 4 )\t \n",
    "# TARGET FOCUS: [ hippo ( x _ 4 ) ] AND dust . [ agent ] ( x _ 5 , [ x _ 4 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ec5122-17f1-4ba4-82f8-b0637718820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 0\n",
      "SUBJECT -----------------------------\n",
      "SUBJ OMISSION/MISPREDICTION: 0\n",
      "AGENT RELATION -----------------------------\n",
      "AGENT ERROR: 0\n",
      "AGENT MISSING: 0\n",
      "AGENT INDEX ERROR: 0\n",
      "THEME RELATION -----------------------------\n",
      "THEME MAPPED TO SUBJECT: 0\n",
      "THEME POSSIBLY INSERTED IN PLACE OF AGENT: 0\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "# targeted check\n",
    "inc_subj = 0  ## count number of cases where main subject is missing/mispredicted\n",
    "subjindex_omission = 0 ## count number of cases where main subject (if present) is missing an index label\n",
    "inc_agent = 0 ## count number of cases where main agent element is incorrect\n",
    "missing_agentrel = 0 ## count number of cases where 'mainverb . agent' element is not present\n",
    "agentrel_indexerror = 0 ## count number of cases where wrong index is used in 'mainverb . agent' element \n",
    "theme_error = 0 ## count number of cases where main subject is incorrectly linked to a theme element \n",
    "mispredicted_rel = 0 ## count number of cases where main agent element may have been mispredicted in form of a theme element\n",
    "\n",
    "\n",
    "for line in inc_cases:\n",
    "    #total+=1\n",
    "    \n",
    "    sent = line[0]\n",
    "    \n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "    \n",
    "    if cat == ' only_seen_as_unacc_subj_as_obj_omitted_transitive_subj':\n",
    "\n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "\n",
    "        for el in target:\n",
    "            if 'hippo' in el:\n",
    "                subj_ind = re.findall(r'\\d+', el)\n",
    "                subj_position = target.index(el)\n",
    "        for el in target:\n",
    "            if ' agent ' in el:\n",
    "                el_ind = el.split(',')\n",
    "                verb_ind = re.findall(r'\\d+', el_ind[0])\n",
    "              \n",
    "        for el in target:\n",
    "            if verb_ind[0] in el and ',' not in el:\n",
    "                main_verb = el.split('(')[0]\n",
    "                   \n",
    "               # el_ind = el.split(',')[1]\n",
    "               # if subj_ind[0] in el_ind:\n",
    "                #    main_verb = el.split('.')[0]\n",
    "\n",
    "        missing_count = 0\n",
    "        for el in pred:\n",
    "            if ' hippo ' not in el and ' * hippo ' not in el:\n",
    "                missing_count+=1\n",
    "                pred_subjind = []\n",
    "            else:\n",
    "                pred_subjind = re.findall(r'\\d+', el) # for ReCOGS main subject index and COGS if index is wrong\n",
    "                \n",
    "            if main_verb in el:\n",
    "                ind = re.findall(r'\\d+', el)\n",
    "                if len(ind) > 0:\n",
    "                    pred_verb_ind = ind[0]\n",
    "            \n",
    "        if len(pred_subjind) == 0:\n",
    "            subjindex_omission+=1\n",
    "        \n",
    "        if missing_count == len(pred):\n",
    "            inc_subj += 1\n",
    "          #  if '.' not in pred[subj_position]:\n",
    "            #    possible_misprediction = pred[subj_position]  ## if hippo is not in LF, using the index of target LF to find the possible element mispredicted in its place\n",
    "             #   pred_subjind = re.findall(r'\\d+', possible_misprediction)\n",
    "              #  print(possible_misprediction)\n",
    "              #  print(pred)\n",
    "\n",
    "        themecount=0\n",
    "        indexerror=0\n",
    "        countmissing_agent = 0\n",
    "       # if f'{main_verb}. agent ' not in line[1]:\n",
    "        #print(pred_verb_ind[0])\n",
    "        if f'agent ( {pred_verb_ind}' not in line[1]:\n",
    "            missing_agentrel+=1\n",
    "            countmissing_agent+=1\n",
    "        for item in pred:\n",
    "            if f'agent ( {pred_verb_ind}' in item:\n",
    "               # if subj_ind[0] not in item: ## Comment out for ReCOGS\n",
    "                #    indexerror+=1\n",
    "                if len(pred_subjind) > 0:\n",
    "                    if pred_subjind[0] not in item:\n",
    "                        indexerror+=1\n",
    "            if 'theme' in item:\n",
    "                if subj_ind[0] in item:\n",
    "                    themecount+=1\n",
    "                  \n",
    "                elif len(pred_subjind) > 0:\n",
    "                    if pred_subjind[0] in item:\n",
    "                        themecount+=1\n",
    "        \n",
    "        if indexerror > 0:\n",
    "            agentrel_indexerror+=1\n",
    "            \n",
    "        if themecount > 0:\n",
    "            theme_error+=1\n",
    "\n",
    "        if missing_agentrel > 0 and themecount > 0:\n",
    "            mispredicted_rel += 1\n",
    "            \n",
    "        \n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "\n",
    "\n",
    "print(f'TOTAL: {len(inc_cases)}')\n",
    "print(f'SUBJECT -----------------------------')\n",
    "print(f'SUBJ OMISSION/MISPREDICTION: {inc_subj}')\n",
    "#print(f'SUBJ MISSING INDEX: {subjindex_omission}')\n",
    "print(f'AGENT RELATION -----------------------------')\n",
    "print(f'AGENT ERROR: {inc_agent}')\n",
    "print(f'AGENT MISSING: {missing_agentrel}')\n",
    "print(f'AGENT INDEX ERROR: {agentrel_indexerror}')\n",
    "#print(f'RELATION VERB MISSING/MISPREDICTED: {relmissing_verb}')\n",
    "print(f'THEME RELATION -----------------------------')\n",
    "print(f'THEME MAPPED TO SUBJECT: {theme_error}')\n",
    "print(f'THEME POSSIBLY INSERTED IN PLACE OF AGENT: {mispredicted_rel}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386bd1e-69a1-4c83-b90f-d108cfe39879",
   "metadata": {},
   "source": [
    "# Object to Subject (Common Noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de68698-af51-4b79-9ca4-a33930abbc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = \"/Users/marina/Desktop/FullReCOGS/[100Epoch]LSTM_77/PRED_77_ende_lstm_recogs_v1_cogs.tsv\"\n",
    "#gen_path = \"/Users/marina/Desktop/RESULTS/R_Transformer/SEED66/PRED_66_ende_transformer_recogs_v1_cogs.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ea4289-c4ca-4728-95f6-635a483768f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mgen_path\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\u001b[39;00m\n\u001b[1;32m      5\u001b[0m seed \u001b[38;5;241m=\u001b[39m run[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m run[\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_path' is not defined"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "#general checks : only predict lexical items present in input, keep track of variable naming (indexing)\n",
    "inserted = 0\n",
    "missing = 0\n",
    "mis_index = 0\n",
    "total = 0\n",
    "incorrect = 0\n",
    "inc_cases = []\n",
    "countmult=0\n",
    "\n",
    "for line in content_t[1:-1]:\n",
    "    #total+=1\n",
    "\n",
    "    sent = line[0]\n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "\n",
    "    if cat == ' obj_to_subj_common':\n",
    "        total+=1\n",
    "    \n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "        \n",
    "        p_elements = []\n",
    "        t_elements = []\n",
    "        \n",
    "        for item in pred:\n",
    "            item = item.split('(')\n",
    "            p_elements.append(item[0])\n",
    "        for item in target:\n",
    "            item = item.split('(')\n",
    "            t_elements.append(item[0])\n",
    "        \n",
    "        case_missing=0\n",
    "        for el in t_elements:\n",
    "            if el not in p_elements:\n",
    "                case_missing+=1\n",
    "              \n",
    "        if case_missing > 0:\n",
    "            missing+=1\n",
    "            \n",
    "\n",
    "        ins_els = []\n",
    "        case_inserted=0\n",
    "        for el in p_elements:\n",
    "            if el not in t_elements:\n",
    "                case_inserted+=1\n",
    "                ins_els.append(el)\n",
    "                \n",
    "        if case_inserted > 0:\n",
    "            inserted+=1\n",
    "\n",
    "        #print(ins_els)  ## to check if there are more than one inserted element in prediction (i.e. more than just the subj element)\n",
    "        #if len(ins_els)>1:\n",
    "         #   print(sent)\n",
    "          #  print(pred)\n",
    "           # print(target)\n",
    "    \n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "        \n",
    "        case_misindex = 0\n",
    "        for group_p, group_t in zip(ind_group_pred, ind_group_target):\n",
    "            if group_p != group_t:\n",
    "                count=0\n",
    "                for i in group_t:\n",
    "                    if 'cockroach' not in i.strip(' '):\n",
    "                        count+=1\n",
    "                case_misindex +=1\n",
    "                \n",
    "        if case_misindex > 1:\n",
    "            countmult+=1\n",
    "    \n",
    "        if case_misindex > 0:\n",
    "            mis_index += 1\n",
    "            \n",
    "        if case_misindex > 0 or case_inserted > 0 or case_missing > 0:\n",
    "            incorrect+=1\n",
    "            inc_cases.append(line)\n",
    "\n",
    "error_pattern = {'total': total, 'incorrect': incorrect, 'missing': missing, 'inserted': inserted, 'index error': mis_index}\n",
    "print(countmult)\n",
    "print(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d29f4-a49d-47cd-a59d-bc6a520fbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overview of Targeted Check\n",
    "\n",
    "## get the groupings (since already have code for that that works on cogs+recogs indexing) and \n",
    "##  output only the groupings with 'agent' and 'theme' roles, compare target to pred, print problem cases and analyse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce88cc8-ada7-4dfb-9d7b-879cf6f784ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mgen_path\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\u001b[39;00m\n\u001b[1;32m      5\u001b[0m seed \u001b[38;5;241m=\u001b[39m run[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m run[\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_path' is not defined"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "# targeted check\n",
    "inc_agent_rel = 0  ## count number of incorrect agent relations\n",
    "inc_theme_rel = 0  ## count number of incorrect theme relations\n",
    "inc_subj = 0   ## count number of cases which omit 'cockroach' subject\n",
    "counttt=0\n",
    "\n",
    "count_agenterror = 0\n",
    "count_themeerror = 0\n",
    "\n",
    "for line in inc_cases:\n",
    "    #total+=1\n",
    "    \n",
    "    sent = line[0]\n",
    "    \n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "    \n",
    "    if cat == ' obj_to_subj_common':\n",
    "\n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "\n",
    "        missing_count = 0\n",
    "        for i in pred:\n",
    "            if ' cockroach ' not in i:\n",
    "                #print(i)\n",
    "                missing_count+=1\n",
    "        \n",
    "        if missing_count == len(pred):\n",
    "            inc_subj += 1\n",
    "\n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "\n",
    "        count = 0\n",
    "        #print()\n",
    "        for t_grouping in ind_group_target:\n",
    "            if ' agent' in t_grouping:\n",
    "                group_ind = ind_group_target.index(t_grouping)\n",
    "                if len(ind_group_pred) < (group_ind+1):\n",
    "                    continue\n",
    "                else:\n",
    "                    pred_grouping = ind_group_pred[group_ind]\n",
    "                    itemcount=0\n",
    "                    if t_grouping != pred_grouping:\n",
    "                        count+=1\n",
    "                        inc_agent_rel+=1\n",
    "\n",
    "            if ' theme' in t_grouping:\n",
    "                group_ind = ind_group_target.index(t_grouping)\n",
    "                if len(ind_group_pred) < (group_ind+1):\n",
    "                    continue\n",
    "                else:\n",
    "                    pred_grouping = ind_group_pred[group_ind]\n",
    "                    if t_grouping != pred_grouping:\n",
    "                        inc_theme_rel+=1\n",
    "               \n",
    "        \n",
    "\n",
    "print(f'TOTAL: {len(inc_cases)}')\n",
    "print(f'SUBJ OMISSION: {inc_subj}')\n",
    "print(f'AGENT ERROR: {inc_agent_rel}')\n",
    "print(count_agenterror)\n",
    "print(f'THEME ERROR: {inc_theme_rel}')\n",
    "print(count_themeerror)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c004d9-5276-408b-97ca-58e7de29ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997945c-0389-4572-8b9a-0492f3f0d587",
   "metadata": {},
   "source": [
    "# Active to Passive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088c60ca-dd01-4721-b141-570bd2e75bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = \"/Users/marina/Desktop/FullReCOGS/[100Epoch]LSTM_77/PRED_77_ende_lstm_recogs_v1_cogs.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdea2fe8-524a-44e2-aaf2-0c419da31015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "795\n",
      "{'total': 1000, 'incorrect': 960, 'missing': 960, 'inserted': 953, 'index error': 960}\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "#general checks : only predict lexical items present in input, keep track of variable naming (indexing)\n",
    "inserted = 0\n",
    "missing = 0\n",
    "mis_index = 0\n",
    "total = 0\n",
    "incorrect = 0\n",
    "inc_cases = []\n",
    "countmult=0\n",
    "\n",
    "countlen = 0\n",
    "for line in content_t[1:-1]:\n",
    "    #total+=1\n",
    "\n",
    "    sent = line[0]\n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "\n",
    "    if cat == ' active_to_passive':\n",
    "        total+=1\n",
    "    \n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "        \n",
    "        p_elements = []\n",
    "        t_elements = []\n",
    "        \n",
    "        for item in pred:\n",
    "            item = item.split('(')\n",
    "            p_elements.append(item[0])\n",
    "        for item in target:\n",
    "            item = item.split('(')\n",
    "            t_elements.append(item[0])\n",
    "        \n",
    "        case_missing=0\n",
    "        for el in t_elements:\n",
    "            if el not in p_elements:\n",
    "                case_missing+=1\n",
    "            \n",
    "        if case_missing > 0:\n",
    "            missing+=1\n",
    "            \n",
    "\n",
    "        ins_els = []\n",
    "        case_inserted=0\n",
    "        for el in p_elements:\n",
    "            if el not in t_elements:\n",
    "                case_inserted+=1\n",
    "                ins_els.append(el)\n",
    "                                \n",
    "        if case_inserted > 0:\n",
    "            inserted+=1\n",
    "\n",
    "        #print(ins_els)  ## to check if there are more than one inserted element in prediction (i.e. more than just the subj element)\n",
    "        #if len(ins_els)>1:\n",
    "         #   print(sent)\n",
    "          #  print(pred)\n",
    "           # print(target)\n",
    "   \n",
    "    \n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "        \n",
    "        case_misindex = 0\n",
    "        for group_p, group_t in zip(ind_group_pred, ind_group_target):\n",
    "            if group_p != group_t:\n",
    "                case_misindex +=1\n",
    "          \n",
    "        if case_misindex > 1:\n",
    "            countmult+=1\n",
    "        if case_misindex > 0:\n",
    "            mis_index += 1\n",
    "        if case_misindex > 0 or case_inserted > 0 or case_missing > 0:\n",
    "            incorrect+=1\n",
    "            inc_cases.append(line)\n",
    "            \n",
    "print(countlen)\n",
    "error_pattern = {'total': total, 'incorrect': incorrect, 'missing': missing, 'inserted': inserted, 'index error': mis_index}\n",
    "print(countmult)\n",
    "print(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3a77928-ad1e-4c45-9d13-e05a2086d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overview of Targeted Check\n",
    "\n",
    "# construction of gen test sentences mostly \"x was blessed by y\", but some are \"x was blessed.\"\n",
    "## idea for targeted check, first look for \"was\" in sentence, then find the word that comes before it (this will should always be\n",
    "##  the main element that in active construction would be agent but here want to make sure its theme), the verb will always be\n",
    "##   \"blessed\" \n",
    "##  so then go to the groupings and find all relational groupings that include bless, check that the theme grouping includes\n",
    "##   the main element we checked for before, also check for agent element and make sure it does NOT include this main passive element\n",
    "\n",
    "# SO blessed_item = x, for pred_grouping: if 'blessed' in pred_grouping and if 'theme' in pred_grouping,\n",
    "##   if blessed_item not in pred_grouping: count // print pred_grouping   ALSO COUNT if theres NO THEME grouping or NO BLESSED\n",
    "## then do the same but for 'agent' in pred_grouping, and if blessed_item IN pred_grouping: count  IF NO AGENT, check sentence for 'by'\n",
    "\n",
    "## ALSO check for incorrect verb prediction, because only one case of 'bless' in training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6572b6c-5590-405e-a383-37e8070826f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 1000\n",
      "VERB OMISSION: 1000\n",
      "THEME ERROR: 0\n",
      "AGENT ERROR: 0\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pprint\n",
    "\n",
    "run = gen_path.split('/')[-1].split('_') # isolates the name of the file, which includes details on the run (ex seed, model, data info), example ['PRED', '42', 'ende', 'transformer', 'recogs', 'v1', 'cogs.tsv']\n",
    "seed = run[1]\n",
    "model = run[3]\n",
    "dataset = run[4]\n",
    "\n",
    "with open(gen_path, 'r') as infile:\n",
    "    content_t = []\n",
    "    rows = infile.read().split('\\n') \n",
    "    for r in rows:\n",
    "        column = r.split('\\t')\n",
    "        content_t.append(column)\n",
    "\n",
    "# targeted check\n",
    "inc_agent_rel = 0  ## count number of incorrect agent relations which include main theme element\n",
    "inc_theme_rel = 0  ## count number of theme relations which incorrectly exclude main theme element\n",
    "inc_verb = 0   ## count number of cases which omit 'cockroach' subject\n",
    "\n",
    "\n",
    "for line in inc_cases:\n",
    "    #total+=1\n",
    "    \n",
    "    sent = line[0]\n",
    "    \n",
    "    pred = line[1]\n",
    "    target = line[2]\n",
    "    cat = line[3]\n",
    "    \n",
    "    if cat == ' active_to_passive':\n",
    "\n",
    "        pred = re.split(';|AND', pred)\n",
    "        target = re.split(';|AND', target)\n",
    "\n",
    "        sent_list = sent.split(\" \")\n",
    "        theme_item_index = sent_list.index(\"was\")-1\n",
    "        theme_item = sent_list[theme_item_index]\n",
    "\n",
    "        misprediciton_list = []\n",
    "        for item in target:\n",
    "            if ' bless' in item:\n",
    "                bless_indx = target.index(item)\n",
    "                misprediction = pred[bless_indx].split('.')[0].strip(' ')\n",
    "                misprediciton_list.append(misprediction)\n",
    "                               \n",
    "        missing_count = 0\n",
    "        for p in pred:\n",
    "            if ' bless ' not in p:\n",
    "                #print(i)\n",
    "                missing_count+=1\n",
    "            else:\n",
    "                for t in target:\n",
    "                    if ' bless . agent' in t and ' bless . agent' not in p:\n",
    "                        missing_count+=1\n",
    "        \n",
    "        if missing_count == len(pred):\n",
    "            inc_verb += 1\n",
    "\n",
    "        ind_group_pred = get_groupings(pred)\n",
    "        ind_group_target = get_groupings(target)\n",
    "        ind_group_pred.sort()\n",
    "        ind_group_target.sort()\n",
    "\n",
    "        for p_grouping in ind_group_pred:\n",
    "            if ' bless ' in p_grouping:\n",
    "                if ' theme' in p_grouping:\n",
    "                    if f' {theme_item}' in p_grouping or f' * {theme_item}' in p_grouping:\n",
    "                        #print(p_grouping)\n",
    "                        yes = 'yes' #not doing anything here, just continuing \n",
    "                    else:\n",
    "                        inc_theme_rel+=1\n",
    "            else:\n",
    "                #inc_theme_rel+=1 ## comment this out if not not wanting to include bless misprediction\n",
    "                for item in misprediciton_list:\n",
    "                    if f' {item} ' in p_grouping:\n",
    "                        if ' theme' in p_grouping:\n",
    "                            if f' {theme_item}' in p_grouping or f' * {theme_item}' in p_grouping:\n",
    "                                yes = 'yes' #not doing anything here, just continuing \n",
    "                                #print(p_grouping)\n",
    "                            else:\n",
    "                                inc_theme_rel+=1\n",
    "                               \n",
    "\n",
    "        count_incagent = 0\n",
    "        for p_grouping in ind_group_pred:\n",
    "            if ' bless ' in p_grouping:\n",
    "                if ' agent' in p_grouping:\n",
    "                    if f' {theme_item}' in p_grouping or f' * {theme_item}' in p_grouping:\n",
    "                        inc_agent_rel+=1\n",
    "            else:\n",
    "                #count_incagent+=1\n",
    "                #inc_agent_rel+=1 ## comment this out if not not wanting to include bless misprediction\n",
    "                for item in misprediciton_list:\n",
    "                    if f' {item} ' in p_grouping:\n",
    "                        if ' agent' in p_grouping:\n",
    "                            #count_incagent+=1\n",
    "                            if f' {theme_item}' in p_grouping or f' * {theme_item}' in p_grouping:\n",
    "                                inc_agent_rel+=1\n",
    "        if count_incagent > 0:\n",
    "            inc_agent_rel+=1\n",
    "                \n",
    "\n",
    "print(f'TOTAL: {len(inc_cases)}')\n",
    "print(f'VERB OMISSION: {inc_verb}')\n",
    "print(f'THEME ERROR: {inc_theme_rel}')\n",
    "print(f'AGENT ERROR: {inc_agent_rel}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
